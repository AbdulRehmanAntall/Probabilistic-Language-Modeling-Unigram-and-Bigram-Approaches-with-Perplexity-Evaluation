# Probabilistic Language Models

This project implements **Unigram and Bigram Language Models**, along with their smoothed versions (Laplace smoothing for unigram, linear interpolation for bigram), to analyze and generate sentences, and evaluate perplexity on test corpora.

---

## ğŸ“‚ Dataset

- **Training Corpus**: 30,000 sentences  
- **Positive Test Corpus**: 1,000 sentences  
- **Negative Test Corpus**: 1000 sentences  
- **Vocabulary Size**: 19,467 unique words  

---

## ğŸ“ Features

<details>
<summary>Click to expand</summary>

1. **Data Preprocessing**:
   - Rare words (frequency < 2) replaced with `UNK`.
   - Added `<s>` (start) and `</s>` (end) tokens for all sentences.
   - Preprocessing for test data ensures unknown words are replaced with `UNK`.

2. **Language Models Implemented**:
   - **Unigram Model**: Counts individual word probabilities.
   - **Smoothed Unigram Model**: Uses Laplace (add-1) smoothing.
   - **Bigram Model**: Counts word pairs (previous word â†’ current word).
   - **Smoothed Bigram Model**: Uses linear interpolation smoothing:
     \[
     P(w_i|w_{i-1}) = \lambda_1 \cdot P_{bigram}(w_i|w_{i-1}) + \lambda_2 \cdot P_{unigram}(w_i)
     \]
     where \(\lambda_1 = \lambda_2 = 0.5\).

3. **Sentence Generation**:
   - Generates sentences using probability distributions.
   - Writes generated sentences along with probabilities to files.

4. **Perplexity Calculation**:
   - Computes perplexity on positive and negative test corpora for all models.
   - Helps evaluate model performance.

</details>

---

## âš™ï¸ Code Structure

<details>
<summary>Click to expand</summary>

- `readFileToCorpus(f)` â†’ Reads file into list of sentences (tokenized).  
- `preprocess(corpus)` â†’ Preprocess training data (replace rare words, add `<s>` and `</s>`).  
- `preprocessTest(vocab, corpus)` â†’ Preprocess test data using training vocabulary.  

### Language Models
- **Parent Class**: `LanguageModel`  
- **Unigram**: `UnigramModel`  
- **Smoothed Unigram**: `SmoothedUnigramModel`  
- **Bigram**: `BigramModel`  
- **Smoothed Bigram**: `SmoothedBigramModelKN`  

### Distributions
- `UnigramDist` â†’ Maintains unigram counts, probability, Laplace probability, and sampling.  
- `BigramDist` â†’ Maintains bigram counts, conditional probabilities, and sampling.

### Main Routine
1. Reads training and test corpora.
2. Preprocesses training and test corpora.
3. Generates vocabulary.
4. Trains language models.
5. Generates 20 sentences per model and stores in files.
6. Computes perplexity for POS and NEG test corpora.

</details>

---

## ğŸ“„ Output Files

- `unigram.txt` â†’ Sentences generated by Unigram Model  
- `unigram_smoothed.txt` â†’ Sentences generated by Smoothed Unigram Model  
- `bigram.txt` â†’ Sentences generated by Bigram Model  
- `bigram_smoothed.txt` â†’ Sentences generated by Smoothed Bigram Model  

---
