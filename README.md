# Probabilistic Language Models

This project implements **Unigram and Bigram Language Models**, along with their smoothed versions (Laplace smoothing for unigram, linear interpolation for bigram), to analyze text, generate sentences, and evaluate model performance using perplexity.

---

## üìÇ Dataset

- **Training Corpus**: 30,000 sentences  
- **Positive Test Corpus**: 1,000 sentences  
- **Negative Test Corpus**: 1,000 sentences  
- **Vocabulary Size**: 19,467 unique words  

---

## üìù Features

<details>
<summary>Click to expand</summary>

### 1. Data Preprocessing
- Rare words (frequency < 2) replaced with `UNK`.
- Added `<s>` (start) and `</s>` (end) tokens to all sentences.
- Test data preprocessing ensures unknown words are replaced with `UNK`.

### 2. Language Models Implemented

1) **Unigram Probability**  
P(w) = count(w) / N
- N = total number of words in corpus

2) **Smoothed Unigram (Laplace Smoothing)**  
P(w) = (count(w) + 1) / (N + V)
- V = vocabulary size  
- Ensures unseen words get non-zero probability

3) **Bigram Probability**  
P(w_i | w_{i-1}) = count(w_{i-1}, w_i) / count(w_{i-1})

4) **Smoothed Bigram (Linear Interpolation)**  
P(w_i | w_{i-1}) = Œª1 * P_bigram(w_i | w_{i-1}) + Œª2 * P_unigram(w_i)
- Œª1 + Œª2 = 1, typically Œª1 = Œª2 = 0.5  
- Smooths bigram probabilities using unigram fallback

### 3. Sentence Generation
- Generates sentences based on model probabilities.
- Saves generated sentences with their probabilities to files.

### 4. Perplexity Calculation
PP(W) = exp( - (1/N) * sum_{i=1}^N log P(w_i | w_{i-1}) )
- Evaluates model performance on positive and negative test corpora  
- Lower perplexity ‚Üí better predictive performance

</details>

---

## ‚öôÔ∏è Code Structure

<details>
<summary>Click to expand</summary>

### Data Handling
- `readFileToCorpus(f)` ‚Üí Reads file into a tokenized list of sentences.  
- `preprocess(corpus)` ‚Üí Replaces rare words, adds `<s>` and `</s>` tokens.  
- `preprocessTest(vocab, corpus)` ‚Üí Ensures test corpus words match training vocabulary.

### Language Models
- **Parent Class**: `LanguageModel`  
- **Unigram**: `UnigramModel`  
- **Smoothed Unigram**: `SmoothedUnigramModel`  
- **Bigram**: `BigramModel`  
- **Smoothed Bigram**: `SmoothedBigramModelKN`  

### Distributions
- `UnigramDist` ‚Üí Maintains word counts, probabilities, Laplace smoothing, and sampling.  
- `BigramDist` ‚Üí Maintains bigram counts, conditional probabilities, and sampling.

### Main Routine
1. Load training and test corpora.  
2. Preprocess corpora and generate vocabulary.  
3. Train all four language models.  
4. Generate 20 sentences per model and save to files.  
5. Compute perplexity for positive and negative test corpora.

</details>

---

## üìÑ Output Files

<details>
<summary>Click to expand</summary>

- `unigram.txt` ‚Üí Sentences generated by Unigram Model  
- `unigram_smoothed.txt` ‚Üí Sentences generated by Smoothed Unigram Model  
- `bigram.txt` ‚Üí Sentences generated by Bigram Model  
- `bigram_smoothed.txt` ‚Üí Sentences generated by Smoothed Bigram Model  

</details>

---

## üîó Notes

<details>
<summary>Click to expand</summary>

- Formulas are implemented correctly in the respective model classes:
  - **Laplace smoothing** for unigrams  
  - **Linear interpolation** for bigrams  
- Perplexity computation ensures probabilities are never zero for smoothed models.  
- Generated sentences provide a qualitative check of model behavior.

</details>
