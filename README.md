# Probabilistic Language Models

This project implements **Unigram and Bigram Language Models**, along with their smoothed versions (Laplace smoothing for unigram, linear interpolation for bigram), to analyze text, generate sentences, and evaluate model performance using perplexity.

---

## 📂 Dataset

- **Training Corpus**: 30,000 sentences  
- **Positive Test Corpus**: 1,000 sentences  
- **Negative Test Corpus**: 1,000 sentences  
- **Vocabulary Size**: 19,467 unique words  

---

## 📝 Features

<details>
<summary>Click to expand</summary>

### 1. Data Preprocessing
- Rare words (frequency < 2) replaced with `UNK`.
- Added `<s>` (start) and `</s>` (end) tokens to all sentences.
- Test data preprocessing ensures unknown words are replaced with `UNK`.

### 2. Language Models Implemented

1) Unigram Probability:
P(w) = count(w) / N

2) Smoothed Unigram (Laplace):
P(w) = (count(w) + 1) / (N + V)

3) Bigram Probability:
P(w_i | w_{i-1}) = count(w_{i-1}, w_i) / count(w_{i-1})

4) Smoothed Bigram (Interpolation):
P(w_i | w_{i-1}) = λ1 * P_bigram(w_i | w_{i-1}) + λ2 * P_unigram(w_i)


### 3. Sentence Generation
- Generates sentences based on model probabilities.
- Saves generated sentences with their probabilities to files.

### 4. Perplexity Calculation
\[
PP(W) = \exp\Big(- \frac{1}{N} \sum_{i=1}^{N} \log P(w_i | w_{i-1})\Big)
\]  
- Evaluates model performance on positive and negative test corpora.
- Lower perplexity → better predictive performance.

</details>

---

## ⚙️ Code Structure

<details>
<summary>Click to expand</summary>

### Data Handling
- `readFileToCorpus(f)` → Reads file into a tokenized list of sentences.  
- `preprocess(corpus)` → Replaces rare words, adds `<s>` and `</s>` tokens.  
- `preprocessTest(vocab, corpus)` → Ensures test corpus words match training vocabulary.

### Language Models
- **Parent Class**: `LanguageModel`  
- **Unigram**: `UnigramModel`  
- **Smoothed Unigram**: `SmoothedUnigramModel`  
- **Bigram**: `BigramModel`  
- **Smoothed Bigram**: `SmoothedBigramModelKN`  

### Distributions
- `UnigramDist` → Maintains word counts, probabilities, Laplace smoothing, and sampling.  
- `BigramDist` → Maintains bigram counts, conditional probabilities, and sampling.

### Main Routine
1. Load training and test corpora.  
2. Preprocess corpora and generate vocabulary.  
3. Train all four language models.  
4. Generate 20 sentences per model and save to files.  
5. Compute perplexity for positive and negative test corpora.

</details>

---

## 📄 Output Files

<details>
<summary>Click to expand</summary>

- `unigram.txt` → Sentences generated by Unigram Model  
- `unigram_smoothed.txt` → Sentences generated by Smoothed Unigram Model  
- `bigram.txt` → Sentences generated by Bigram Model  
- `bigram_smoothed.txt` → Sentences generated by Smoothed Bigram Model  

</details>

---

## 🔗 Notes

<details>
<summary>Click to expand</summary>

- Formulas are implemented correctly in the respective model classes:
  - **Laplace smoothing** for unigrams  
  - **Linear interpolation** for bigrams  
- Perplexity computation ensures probabilities are never zero for smoothed models.  
- Generated sentences provide a qualitative check of model behavior.

</details>
